{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## create TFrecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtx/local/anaconda2/envs/tf1.5/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirbase='/nfs/P100/SDSSV_Classifiers/processed_dataset/TF_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define corresponding data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the following functions for different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to(data_set, Dir, name):\n",
    "    \"\"\"Converts a dataset to tfrecords.\"\"\"\n",
    "    features = data_set.features\n",
    "    labels = data_set.labels\n",
    "    num_examples = data_set.num_examples\n",
    "\n",
    "    if features.shape[0] != num_examples:\n",
    "        raise ValueError('Features size %d does not match label size %d.' %\n",
    "                         (features.shape[0], num_examples))\n",
    "\n",
    "    filename = os.path.join(Dir, name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in xrange(num_examples):\n",
    "        data_set.index = index\n",
    "        feature_raw = features[index].reshape(-1).tolist()\n",
    "        label_raw = labels[index].reshape(-1).tolist()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'index': _int64_feature(data_set.index),\n",
    "            'label_raw': _float_feature(label_raw),\n",
    "            'feature_raw': _float_feature(feature_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, filename):\n",
    "    # Get the data.\n",
    "    class data_set():\n",
    "        pass\n",
    "    Len = data.shape[0]\n",
    "    features = data['flux_norm']\n",
    "    labels = data['label'].reshape(-1,1)\n",
    "  \n",
    "    data_set.features = features\n",
    "    data_set.labels = labels\n",
    "    data_set.num_examples = Len\n",
    "\n",
    "    convert_to(data_set, Dir=dirbase, name=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the convert code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_type = ['wdsb2', 'wd', 'fgkm', 'hotstars', 'yso', 'cv']\n",
    "Dir = '/nfs/P100/SDSSV_Classifiers/processed_dataset/dataset'\n",
    "mean = np.load('/nfs/P100/SDSSV_Classifiers/processed_dataset/Norm_mu.npy')\n",
    "std = np.load('/nfs/P100/SDSSV_Classifiers/processed_dataset/Norm_std.npy')\n",
    "def loaddata(mode='train'):\n",
    "    DATA = []\n",
    "    for i in star_type:\n",
    "        filename=os.path.join(Dir, mode+'_'+i+'.npy')\n",
    "        data = np.load(filename)\n",
    "        data['flux_norm'] = (data['flux_norm']-mean)/std #!!!\n",
    "        DATA.append(data)\n",
    "    DATA = np.hstack(DATA)\n",
    "    random_ind = np.random.permutation(np.arange(DATA.shape[0]))\n",
    "    DATA = DATA[random_ind]\n",
    "    print mode, DATA['flux_norm'].mean(), DATA['flux_norm'].std()\n",
    "    return DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -0.03525323 2.3771825\n",
      "valid -0.0004990322 1.1612086\n",
      "test 0.002460175 3.8226826\n"
     ]
    }
   ],
   "source": [
    "train_dataset = loaddata('train')\n",
    "valid_dataset = loaddata('valid')\n",
    "test_dataset = loaddata('test')\n",
    "Dir2 = '/nfs/P100/SDSSV_Classifiers/processed_dataset/TF_dataset'\n",
    "np.save(os.path.join(Dir2,'train.npy'),train_dataset)\n",
    "np.save(os.path.join(Dir2,'valid.npy'),valid_dataset)\n",
    "np.save(os.path.join(Dir2,'test.npy'),test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Writing', '/nfs/P100/SDSSV_Classifiers/processed_dataset/TF_dataset/training.tfrecords')\n",
      "('Writing', '/nfs/P100/SDSSV_Classifiers/processed_dataset/TF_dataset/valid.tfrecords')\n",
      "('Writing', '/nfs/P100/SDSSV_Classifiers/processed_dataset/TF_dataset/test.tfrecords')\n"
     ]
    }
   ],
   "source": [
    "main(train_dataset,'training')\n",
    "main(valid_dataset,'valid')\n",
    "main(test_dataset,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check repeat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1778,)\n",
      "(1779,)\n",
      "(40506,)\n"
     ]
    }
   ],
   "source": [
    "print test_dataset.shape\n",
    "print valid_dataset.shape\n",
    "print train_dataset.shape\n",
    "a = valid_dataset\n",
    "b = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.unique(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('index', '<i4'), ('label', '<i4'), ('flux_norm', '<f4', (3000,))]\n",
      "(14250,)\n"
     ]
    }
   ],
   "source": [
    "print s.dtype\n",
    "print s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(a)):\n",
    "    for j in xrange(len(s)):\n",
    "        if a[i]['index'] != s[j]['index']:\n",
    "            continue\n",
    "        if a[i]['label'] != s[j]['label']:\n",
    "            continue\n",
    "        if (a[i]['flux_norm'][:20] == s[j]['flux_norm'][:20]).sum() == 0:\n",
    "            continue\n",
    "        print i,j\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
